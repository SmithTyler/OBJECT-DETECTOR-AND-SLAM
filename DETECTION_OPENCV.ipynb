{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e35611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs \n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c84526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapped_detection(in_image):\n",
    "    \n",
    "    #Loading the model \n",
    "    net = cv2.dnn.readNet('7_class_5k.onnx')\n",
    "    \n",
    "    #Using cuda cores\n",
    "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA_FP16)\n",
    "    \n",
    "    #FORMATTING THE IMAGE TO YOLOv5's 640X640 input\n",
    "    row, col = in_image.shape[:2]\n",
    "    expected = 640\n",
    "    aspect = col / row\n",
    "    resized_image  = cv2.resize(in_image, (round(expected * aspect), expected))\n",
    "    crop_start = round(expected * (aspect - 1) / 2)\n",
    "    crop_img = resized_image[0:expected, crop_start:crop_start+expected]\n",
    "    \n",
    "    #NEW RESIZE\n",
    "    dim = (640, 640)\n",
    "    resize = cv2.resize(in_image, dim)\n",
    "    \n",
    "    #Creating a blob from img to pass into net\n",
    "    blob = cv2.dnn.blobFromImage(crop_img,1/255.0,(640, 640), swapRB=True)\n",
    "    \n",
    "    #Passing the image through net\n",
    "    net.setInput(blob)\n",
    "    predictions = net.forward()\n",
    "    \n",
    "    #Extracting the resulting matrix\n",
    "    output_data = predictions[0]\n",
    "    \n",
    "    return resize, crop_img, output_data\n",
    "    \n",
    "\n",
    "###############################################################\n",
    "#This function unwraps the detection: gets boxes, conf, classes \n",
    "###############################################################\n",
    "def unwrap_detection(input_image, output_data):\n",
    "    \n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    rows = output_data.shape[0]\n",
    "\n",
    "    image_width, image_height, _ = input_image.shape\n",
    "\n",
    "    x_factor = image_width / 640\n",
    "    y_factor =  image_height / 640\n",
    "\n",
    "    for r in range(rows):\n",
    "        row = output_data[r]\n",
    "        confidence = row[4]\n",
    "        if confidence >= 0.4:\n",
    "\n",
    "            classes_scores = row[5:]\n",
    "            _, _, _, max_indx = cv2.minMaxLoc(classes_scores)\n",
    "            class_id = max_indx[1]\n",
    "            if (classes_scores[class_id] > .25):\n",
    "\n",
    "                confidences.append(confidence)\n",
    "\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "                x, y, w, h = row[0].item(), row[1].item(), row[2].item(), row[3].item() \n",
    "                left = int((x - 0.5 * w) * x_factor)\n",
    "                top = int((y - 0.5 * h) * y_factor)\n",
    "                width = int(w * x_factor)\n",
    "                height = int(h * y_factor)\n",
    "                box = np.array([left, top, width, height])\n",
    "                boxes.append(box)\n",
    "                \n",
    "                \n",
    "            \n",
    "                #Removing the overlapping/duplicated detections\n",
    "                indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.15, 0.25)\n",
    "                \n",
    "                #Remaking the detections by only keeping true indexes\n",
    "                result_class_ids = []\n",
    "                result_confidences = []\n",
    "                result_boxes = []\n",
    "                \n",
    "                for i in indexes:\n",
    "                    result_class_ids.append(class_ids[i])\n",
    "                    result_confidences.append(confidences[i])\n",
    "                    result_boxes.append(boxes[i])\n",
    "    if len(class_ids) == 0:\n",
    "        result_class_ids = 0\n",
    "        result_confidences = 0\n",
    "        result_boxes = 0\n",
    "        return result_class_ids, result_confidences, result_boxes\n",
    "    else: \n",
    "        return result_class_ids, result_confidences, result_boxes\n",
    "\n",
    "def print_box(image,result_class_ids, result_confidences, result_boxes):\n",
    "    classNames = (\"APPLE\",\n",
    "                  \"BELL PEPPER\",\n",
    "                  \"CORN\",\n",
    "                  \"PEACH\",\n",
    "                  \"POTATO\",\n",
    "                  \"RASBERRY\",\n",
    "                  \"TOMATO\")\n",
    "    for i in range(len(result_class_ids)):\n",
    "\n",
    "        box = result_boxes[i]\n",
    "        class_id = result_class_ids[i]\n",
    "\n",
    "        cv2.rectangle(image, box, (0, 255, 255), 2)\n",
    "        cv2.rectangle(image, (box[0], box[1] - 20), (box[0] + box[2], box[1]), (0, 255, 255), -1)\n",
    "        cv2.putText(image, classNames[class_id]+' '+str(round(CONF[i],3)), (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0))\n",
    "    \n",
    "    #plt.figure(figsize=(30,30))\n",
    "    #plt.imshow(image)\n",
    "    #plt.show()\n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa208e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETECTION !!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    img = cv2.imread('APPLE.jpg')\n",
    "    #plt.imshow(img)\n",
    "    #Calling Wrapped Detection\n",
    "    resize,crop_img, output_data = wrapped_detection(img)\n",
    "    #print(output_data)\n",
    "    #Calling Unwrapped Detection\n",
    "    IDS, CONF, BOXES = unwrap_detection(img, output_data)\n",
    "    #print(IDS)\n",
    "    if (IDS !=0)&(CONF !=0)&(BOXES!=0):\n",
    "        print('DETECTION !!')\n",
    "        image = print_box(img, IDS, CONF, BOXES)\n",
    "        os.remove('DETECTION.jpg')\n",
    "        cv2.imwrite('DETECTION.jpg',image)\n",
    "    else:\n",
    "        print('NO DETECTION')\n",
    "    #Printing the boxes\n",
    "    #print_box(img, IDS, CONF, BOXES)\n",
    "    #print(CONF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae2936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_main(filename):\n",
    "    img = cv2.imread(filename)\n",
    "\n",
    "    #Calling Wrapped Detection\n",
    "    resize,crop_img, output_data = wrapped_detection(img)\n",
    "    #print(output_data)\n",
    "    #Calling Unwrapped Detection\n",
    "    IDS, CONF, BOXES = unwrap_detection(img, output_data)\n",
    "    #print(IDS)\n",
    "    if (IDS !=0)&(CONF !=0)&(BOXES!=0):\n",
    "        print('DETECTION !!')\n",
    "        image = print_box(img, IDS, CONF, BOXES)\n",
    "        os.remove('DETECTION.jpg')\n",
    "        cv2.imwrite('DETECTION.jpg',image)\n",
    "        return IDS, CONF\n",
    "    else:\n",
    "        print('NO DETECTION')\n",
    "        \n",
    "        return 0,0\n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2f8a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETECTION !!\n",
      "[0, 0]\n",
      "[0.6923275, 0.50071764]\n"
     ]
    }
   ],
   "source": [
    "IDS, CONF =detection_main('APPLE.jpg')\n",
    "print(IDS)\n",
    "print(CONF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
